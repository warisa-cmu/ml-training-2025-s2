{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a16f68b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pprint import pp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2caedfd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Residual plots\n",
    "IS_PLOT_RES = False\n",
    "SAVE_PLOT_RES = False\n",
    "\n",
    "# Combined plots\n",
    "SAVE_PLOT = False\n",
    "\n",
    "# Data\n",
    "SAVE_DATA = False"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "576bff49",
   "metadata": {},
   "source": [
    "### Class definitions\n",
    "Note that in real production you should put this definition in a separate .py file and import it. \n",
    "For example\n",
    "\n",
    "```python\n",
    "from classes_v1 import DataHandler, MyUtil, RegSwitcher\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad46cd8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "from datetime import datetime\n",
    "from sklearn.base import BaseEstimator\n",
    "from sklearn.model_selection import train_test_split\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import (\n",
    "    PredictionErrorDisplay,\n",
    "    mean_absolute_percentage_error,\n",
    "    mean_squared_error,\n",
    "    r2_score,\n",
    ")\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "class MyUtil:\n",
    "    @staticmethod\n",
    "    def save_data(filename, data):\n",
    "        with open(filename, \"wb\") as file:\n",
    "            pickle.dump(data, file)\n",
    "    \n",
    "    @staticmethod\n",
    "    def load_data(filename):\n",
    "        with open(filename, \"rb\") as file:\n",
    "            data = pickle.load(file)\n",
    "        return data\n",
    "\n",
    "    @staticmethod\n",
    "    def get_dt():\n",
    "        return datetime.now().strftime(\"%Y-%m-%d_%H-%M\")\n",
    "\n",
    "\n",
    "class DataHandler:\n",
    "    def __init__(self, _X, _Y, scalerX, scalerY):\n",
    "        self._X = _X\n",
    "        self._Y = _Y\n",
    "        self.scalerX = scalerX\n",
    "        self.scalerY = scalerY\n",
    "        self.X_train = None\n",
    "        self.X_test = None\n",
    "        self.Y_train = None\n",
    "        self.Y_test = None\n",
    "\n",
    "    def split_and_scale(self, test_size, random_state):\n",
    "        _X_train, _X_test, _Y_train, _Y_test = train_test_split(\n",
    "            self._X, self._Y, test_size=test_size, random_state=random_state\n",
    "        )\n",
    "        self.X_train = self.scalerX.fit_transform(_X_train)\n",
    "        self.X_test = self.scalerX.transform(_X_test)\n",
    "\n",
    "        self.Y_train = self.scalerY.fit_transform(_Y_train)\n",
    "        self.Y_test = self.scalerY.transform(_Y_test)\n",
    "\n",
    "    def get_train(self):\n",
    "        return self.X_train, self.Y_train\n",
    "\n",
    "    def get_test(self):\n",
    "        return self.X_test, self.Y_test\n",
    "\n",
    "\n",
    "class RegSwitcher(BaseEstimator):\n",
    "    def __init__(self, base=None):\n",
    "        self.base = base\n",
    "        self.dt = datetime.now().strftime(\"%Y-%m-%d_%H-%M\")\n",
    "\n",
    "    def fit(self, X, Y):\n",
    "        self.base.fit(X, Y)\n",
    "        self.is_fitted_ = True\n",
    "        return self\n",
    "\n",
    "    def predict(self, X):\n",
    "        return self.base.predict(X)\n",
    "\n",
    "\n",
    "class MyEval:\n",
    "    @staticmethod\n",
    "    def eval_perf(y_true, y_pred):\n",
    "        mse = mean_squared_error(y_true=y_true, y_pred=y_pred)\n",
    "        mape = mean_absolute_percentage_error(y_true=y_true, y_pred=y_pred)\n",
    "        r2 = r2_score(y_true=y_true, y_pred=y_pred)\n",
    "        return mse, mape, r2\n",
    "\n",
    "    @classmethod\n",
    "    def eval(cls, Y_train, Y_test, Y_train_pred, Y_test_pred, **kwargs):\n",
    "        data_arr = []\n",
    "        for i in range(0, Y_train.shape[1]):\n",
    "            mse_train, mape_train, r2_train = cls.eval_perf(\n",
    "                y_true=Y_train[:, i], y_pred=Y_train_pred[:, i]\n",
    "            )\n",
    "            mse_test, mape_test, r2_test = cls.eval_perf(\n",
    "                y_true=Y_test[:, i], y_pred=Y_test_pred[:, i]\n",
    "            )\n",
    "\n",
    "            data = {\n",
    "                **kwargs,\n",
    "                \"Y\": f\"Y-{i + 1}\",\n",
    "                \"MSE Train (No Val)\": mse_train,\n",
    "                \"MSE Test\": mse_test,\n",
    "                \"MAPE Train (No Val)\": mape_train,\n",
    "                \"MAPE Test\": mape_test,\n",
    "                \"R2 Train (No Val)\": r2_train,\n",
    "                \"R2 Test\": r2_test,\n",
    "            }\n",
    "            data_arr.append(data)\n",
    "\n",
    "        mse_train, mape_train, r2_train = cls.eval_perf(\n",
    "            y_true=Y_train, y_pred=Y_train_pred\n",
    "        )\n",
    "        mse_test, mape_test, r2_test = cls.eval_perf(y_true=Y_test, y_pred=Y_test_pred)\n",
    "        data = {\n",
    "            **kwargs,\n",
    "            \"Y\": \"Y-All\",\n",
    "            \"MSE Train (No Val)\": mse_train,\n",
    "            \"MSE Test\": mse_test,\n",
    "            \"MAPE Train (No Val)\": mape_train,\n",
    "            \"MAPE Test\": mape_test,\n",
    "            \"R2 Train (No Val)\": r2_train,\n",
    "            \"R2 Test\": r2_test,\n",
    "        }\n",
    "        data_arr.append(data)\n",
    "        df_eval = pd.DataFrame.from_dict(data_arr)\n",
    "        return df_eval\n",
    "\n",
    "    @classmethod\n",
    "    def plot_res(\n",
    "        cls,\n",
    "        Y_train,\n",
    "        Y_test,\n",
    "        Y_train_pred,\n",
    "        Y_test_pred,\n",
    "        current_dir=\".\",\n",
    "        dt=\"\",\n",
    "        save=False,\n",
    "        show=False,\n",
    "        file_prefix=\"\",\n",
    "    ):\n",
    "        for i in range(0, Y_train.shape[1]):\n",
    "            fig, axes = plt.subplots(\n",
    "                nrows=1,\n",
    "                ncols=2,\n",
    "                figsize=(10, 5),\n",
    "                constrained_layout=True,\n",
    "                sharex=True,\n",
    "                sharey=True,\n",
    "            )\n",
    "\n",
    "            display_train = PredictionErrorDisplay(\n",
    "                y_true=Y_train[:, i], y_pred=Y_train_pred[:, i]\n",
    "            )\n",
    "            display_train.plot(ax=axes[0])\n",
    "            axes[0].set_title(\"Train\")\n",
    "\n",
    "            display_train = PredictionErrorDisplay(\n",
    "                y_true=Y_test[:, i], y_pred=Y_test_pred[:, i]\n",
    "            )\n",
    "            display_train.plot(ax=axes[1])\n",
    "            axes[1].set_title(\"Test\")\n",
    "\n",
    "            if file_prefix != \"\":\n",
    "                fig.suptitle(file_prefix)\n",
    "\n",
    "            if save:\n",
    "                if file_prefix == \"\":\n",
    "                    raise Exception(\"Please specify file prefix\")\n",
    "\n",
    "                filename = f\"{current_dir}/{file_prefix}_{dt}_{i}.png\"\n",
    "                fig.savefig(filename, dpi=300)\n",
    "\n",
    "            if show:\n",
    "                plt.show()\n",
    "            else:\n",
    "                plt.close(fig)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c4f9b2c",
   "metadata": {},
   "source": [
    "### Load data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80adc226",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Search for pkl files\n",
    "from os import listdir\n",
    "from os.path import isfile, join\n",
    "\n",
    "onlyfiles = [f for f in listdir(\".\") if (isfile(join(\".\", f)) and f.endswith(\"pkl\"))]\n",
    "pp(onlyfiles)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5230884e",
   "metadata": {},
   "outputs": [],
   "source": [
    "filenameInput = \"S04_data_2025-12-11_11-50.pkl\"\n",
    "data_load = MyUtil.load_data(filename=filenameInput)\n",
    "\n",
    "# Print keys\n",
    "pp([k for k in data_load.keys()])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "123a1774",
   "metadata": {},
   "outputs": [],
   "source": [
    "dt = MyUtil.get_dt()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5402e27f",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_handler = data_load[\"data_handler\"]\n",
    "df_cv = data_load[\"df_cv\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4757f361",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_cv"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "959fb424",
   "metadata": {},
   "source": [
    "### Calculate test results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1782a10d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sort the DataFrame by \"rank_test_score\"\n",
    "df_cv = df_cv.sort_values(by=\"rank_test_score\")\n",
    "\n",
    "# Groups the sorted DataFrame by the columns \"id_split\" and \"estimator\".\n",
    "# For each group (unique combination of split and estimator), selects the first row (which, after sorting, is the one with the best rank_test_score).\n",
    "# .reset_index() turns the groupby indices back into columns for a clean DataFrame.\n",
    "df_fit_select = df_cv.groupby([\"id_split\", \"estimator\"]).first().reset_index()\n",
    "\n",
    "display(df_fit_select)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76b1e55b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize blank model (optional)\n",
    "reg = RegSwitcher(base=None)\n",
    "\n",
    "\n",
    "df_arr = []\n",
    "for idx, fit in df_fit_select.iterrows():\n",
    "    # pp(fit[\"param_split\"])\n",
    "    # pp(fit[\"params\"])\n",
    "\n",
    "    param_split = fit[\"param_split\"]\n",
    "    data_handler.split_and_scale(**param_split)\n",
    "\n",
    "    X_train, Y_train = data_handler.get_train()\n",
    "    X_test, Y_test = data_handler.get_test()\n",
    "\n",
    "    params = fit[\"params\"]\n",
    "    reg.set_params(**params)\n",
    "\n",
    "    reg.fit(X_train, Y_train)\n",
    "\n",
    "    Y_train_pred = reg.predict(X_train)\n",
    "    Y_test_pred = reg.predict(X_test)\n",
    "\n",
    "    _df = MyEval.eval(\n",
    "        Y_train=Y_train,\n",
    "        Y_train_pred=Y_train_pred,\n",
    "        Y_test=Y_test,\n",
    "        Y_test_pred=Y_test_pred,\n",
    "        id_split=fit[\"id_split\"],\n",
    "        id_gs=fit[\"id_gs\"],\n",
    "        estimator=fit[\"estimator\"],\n",
    "    )\n",
    "    df_arr.append(_df)\n",
    "\n",
    "    if IS_PLOT_RES:\n",
    "        id_split = fit[\"id_split\"]\n",
    "        estimator = fit[\"estimator\"]\n",
    "        MyEval.plot_res(\n",
    "            Y_train=Y_train,\n",
    "            Y_train_pred=Y_train_pred,\n",
    "            Y_test=Y_test,\n",
    "            Y_test_pred=Y_test_pred,\n",
    "            dt=dt,\n",
    "            save=SAVE_PLOT_RES,\n",
    "            file_prefix=f\"S05-{estimator}-{id_split}\",\n",
    "        )\n",
    "\n",
    "df_eval = pd.concat(df_arr).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83dd74b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_eval"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8bb7e1e2",
   "metadata": {},
   "source": [
    "### Further data processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "628d6777",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge\n",
    "# - **After this code runs:**\n",
    "#   - `df_eval` will have a new column, `validation_scores`, containing values from `df_cv` where the keys match.\n",
    "#   - All original rows in `df_eval` are preserved.\n",
    "\n",
    "# - **What it does:**\n",
    "#   - Sets the value of the `\"Y\"` column in the `df_cv` DataFrame to `\"Y-All\"` for all rows.\n",
    "#   - If the `\"Y\"` column does not exist, it is created.\n",
    "df_cv[\"Y\"] = \"Y-All\"\n",
    "\n",
    "# - **What it does:**\n",
    "#   - Defines a list of column names that will be used from `df_cv` when merging with `df_eval`.\n",
    "#   - These columns are: `id_split`, `id_gs`, `Y`, and `validation_scores`.\n",
    "colsToMerge = [\"id_split\", \"id_gs\", \"Y\", \"validation_scores\"]\n",
    "\n",
    "# - **What it does:**\n",
    "#   - Merges the `df_eval` DataFrame with a subset of `df_cv` (only the columns in `colsToMerge`).\n",
    "#   - The merge is performed on the columns `id_split`, `id_gs`, and `Y`.\n",
    "#   - `how=\"left\"` specifies a left join, meaning all rows from `df_eval` are kept, and matching rows from `df_cv` are added where available.\n",
    "#   - If there is no match in `df_cv`, the new columns (e.g., `validation_scores`) will have `NaN` values in `df_eval`.\n",
    "df_eval = df_eval.merge(df_cv[colsToMerge], on=[\"id_split\", \"id_gs\", \"Y\"], how=\"left\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52dd2af0",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_eval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27f2902c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "\n",
    "# - **What this function does:**\n",
    "#   - Takes a DataFrame `_df` (expected to be a group from a groupby operation).\n",
    "#   - Extracts the `\"validation_scores\"` column, which contains arrays or lists.\n",
    "#   - Concatenates all arrays/lists into a single NumPy array (`val_scores`).\n",
    "#   - Returns a new DataFrame with one column, `\"cv_results\"`, containing all the concatenated scores.\n",
    "def expandCV(_df):\n",
    "    val_scores = np.concatenate(_df[\"validation_scores\"].values)\n",
    "    return pd.DataFrame(data={\"cv_results\": val_scores})\n",
    "\n",
    "\n",
    "#   - Creates a boolean mask that is `True` where the `\"Y\"` column equals `\"Y-All\"`.\n",
    "#   - This mask is used to filter the DataFrame for relevant rows.\n",
    "filt = df_eval[\"Y\"] == \"Y-All\"\n",
    "\n",
    "# - **Step-by-step:**\n",
    "#   1. **Filter Rows:**\n",
    "#      `df_eval[filt]` selects only rows where `\"Y\" == \"Y-All\"`.\n",
    "#   2. **Group by Estimator:**\n",
    "#      `.groupby(by=[\"estimator\"])` groups the filtered DataFrame by the `\"estimator\"` column.\n",
    "#   3. **Apply `expandCV`:**\n",
    "#      `.apply(expandCV, include_groups=False)` applies the `expandCV` function to each group, expanding the cross-validation results into individual rows.\n",
    "#   4. **Reset Index:**\n",
    "#      `.reset_index(drop=False)` resets the index so that group labels become columns.\n",
    "#   5. **Drop Extra Column:**\n",
    "#      `.drop(columns=[\"level_1\"])` removes the `\"level_1\"` column, which is a byproduct of the groupby-apply process.\n",
    "cv_data = (\n",
    "    df_eval[filt]\n",
    "    .groupby(by=[\"estimator\"])\n",
    "    .apply(expandCV)\n",
    "    .reset_index(drop=False)\n",
    "    .drop(columns=[\"level_1\"])\n",
    ")\n",
    "\n",
    "display(cv_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13189e5f",
   "metadata": {},
   "source": [
    "### Plotting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73a11f5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "fig, axes = plt.subplots(1, 3, figsize=(20, 4))\n",
    "\n",
    "# Plot CV results\n",
    "sns.boxplot(cv_data, x=\"estimator\", y=\"cv_results\", ax=axes[0])\n",
    "axes[0].set_ylim([0, 1])\n",
    "axes[0].set_title(\"CV Results\")\n",
    "\n",
    "# Plot test results\n",
    "ax = sns.boxplot(data=df_eval, x=\"estimator\", y=\"R2 Test\", hue=\"Y\", ax=axes[1])\n",
    "axes[1].set_title(\"Test Result\")\n",
    "\n",
    "# Plot train (no cv) results\n",
    "sns.boxplot(data=df_eval, x=\"estimator\", y=\"R2 Train (No Val)\", hue=\"Y\", ax=axes[2])\n",
    "axes[2].set_title(\"Train Result (No Validation)\")\n",
    "\n",
    "if SAVE_PLOT:\n",
    "    filename = f\"S05_eval_{dt}.png\"\n",
    "    fig.savefig(filename, dpi=300, bbox_inches=\"tight\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b3d579a",
   "metadata": {},
   "source": [
    "### Save data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11e1883a",
   "metadata": {},
   "outputs": [],
   "source": [
    "if SAVE_DATA:\n",
    "    filename = f\"S05_data_{dt}.pkl\"\n",
    "\n",
    "    data_save = {\n",
    "        \"desc\": \"This is the saved data\",\n",
    "        \"filenameInput\": filename,\n",
    "        \"df_eval\": df_eval,\n",
    "        \"cv_data\": cv_data,\n",
    "    }\n",
    "\n",
    "    # Save the model\n",
    "    MyUtil.save_data(filename=filename, data=data_save)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43dc7078",
   "metadata": {},
   "source": [
    "### Test loading data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33e99991",
   "metadata": {},
   "outputs": [],
   "source": [
    "if SAVE_DATA:\n",
    "    data_load = MyUtil.load_data(filename=filename)\n",
    "\n",
    "    pp(list(data_load.keys()))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "admin",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
